import tensorflow as tf
import matplotlib.pyplot as plt

X = tf.constant([0, 0.5, 1.0, 1.5, 2.0, 2.5]) 
Y = tf.constant([0.3, 1.9, 2.4, 4.1, 6.8, 7.9])
a0 = tf.Variable(tf.random.uniform([1], -1.0, 1.0))
a1 = tf.Variable(tf.random.uniform([1], -1.0, 1.0))
a2 = tf.Variable(tf.random.uniform([1], -1.0, 1.0))

optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)
#optimizer =tf.optimizers.Adam(learning_rate=0.07, beta_1=0.9, beta_2=0.999, 
#                               epsilon=1e-08, amsgrad=False, name='Adam')
#optimizer =tf.optimizers.SGD(learning_rate=0.01)

@tf.function()
def cost_eval():
   # y = a0+a1*x+a2*x^2 (2차함수)
   # W 와 X 가 행렬이 아니므로 tf.matmul 이 아니라 기본 곱셈 기호를 사용 
    hypothesis = a0+a1*X+(a2*(X**2))
    cost = tf.reduce_mean(tf.square(hypothesis - Y))
    return cost
    
print("epoch    a0         a1            a2         cost")
 # 최적화를 10번 수행
for epoch in range(0,10, 1):
      
        # 손실 함수를 작성.
        # mean(h - Y)^2 : 예측값과 실제값의 거리를 비용(손실) 함수로 계산.
        
        # 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 최적화를 수행
        # 비용을 최소화 하는 것이 최종 목표
        optimizer.minimize(cost_eval, var_list=[a0,a1,a2])
       # 손실, 가중치, 편향을 출력
        print(epoch,  a0.numpy(), a1.numpy(), a2.numpy(), cost_eval().numpy())
        # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인.
print("\n=== Test ===")
x=5.
print('X:', x, 'Y:', (a0+a1*x+(a2*(x**2))).numpy())
x=2.5
print('X:', x, 'Y:', (a0+a1*x+(a2*(x**2))).numpy())


# 그래프 그리기 위해 새로운 X값을 입력
new_X = tf.range(0, 3, 0.05)
# 선형 회귀직선을 이용하여 예측 Y값 계산
new_Y = a0+a1*new_X+(a2*(new_X**2))

plt.plot(X,Y,'ro', label='Sample Data') # 'ro'는 red circle
plt.plot(new_X, new_Y,'b-')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()
