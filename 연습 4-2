import tensorflow as tf
import matplotlib.pyplot as plt

#X 입력값
#Y X에 대해 예측된 y의 값
X = [0, 0.5, 1.0, 1.5, 2.0, 2.5] 
Y = [0.3, 1.9, 2.4, 4.1, 6.8, 7.9]
# W, b를 -1.0~1.0 사이에 정규분포 갖는 1차원 배열로 설정
W = tf.Variable(tf.random.uniform([1], -1.0, 1.0))
b = tf.Variable(tf.random.uniform([1], -1.0, 1.0))

# 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 경사 하강법 최적화를 수행
#학습률은 0.1로 고정
optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)

# 비용 함수 계산
@tf.function()
def cost_eval():
   # y = W(가중치 ) * x + b (bias , 편향)
   # W 와 X 가 행렬이 아니므로 tf.matmul 이 아니라 기본 곱셈 기호를 사용 
    hypothesis = W * X + b
   # mean(h - Y)^2 : 예측값과 실제값의 거리를 비용(손실) 함수로 계산.
    cost = tf.reduce_mean(tf.square(hypothesis - Y))
    return cost
    
print("epoch    W          b       cost")
# 최적화를 10번 수행
for epoch in range(10):
        
        # 비용을 최소화 하는 것이 최종 목표
        optimizer.minimize(cost_eval, var_list=[W,b])
       # 손실, 가중치, 편향을 출력
       # numpy()함수는 변수의 배열만 출력
        print(epoch,  W.numpy(), b.numpy(), cost_eval().numpy())
        # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인.
print("\n=== Test ===")
x=5.
print('X:', x, 'Y:', (W * x + b).numpy())
x=2.5
print('X:', x, 'Y:', (W * x + b).numpy())

# 그래프 그리기 위해 새로운 X값을 입력
new_X = tf.range(0, 3, 0.05)
# 선형 회귀직선을 이용하여 예측 Y값 계산
new_Y = W*new_X+b

plt.plot(X,Y,'ro', label='Sample Data') # 'ro'는 red circle
plt.plot(new_X, new_Y,'b-')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()
